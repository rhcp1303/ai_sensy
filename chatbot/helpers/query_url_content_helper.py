from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
import os
from ..helpers.prompt_helper import prompt_template
import logging

logger = logging.getLogger(__name__)

api_key = "AIzaSyBq2_GdMf0KhowSVSb0hn4Z_8B81kBewXY"
os.environ["GOOGLE_API_KEY"] = api_key


def answer_question(query, vectorstore):
    """
        Answers a question based on the provided query and vector database.

        This function uses LangChain's RetrievalQA chain with a "stuff" chain type and a
        Google Generative AI model (gemini-1.5-flash) to answer questions. It retrieves
        relevant documents from the vector database and uses a custom prompt template
        to format the prompt for the language model.

        Args:
            query (str): The question to be answered.
            vectorstore (langchain.vectorstores.VectorStore): The vector database containing
                                                          the document embeddings.  This should be an
                                                          instance of a LangChain VectorStore class.

        Returns:
            str: The answer generated by the language model, or an error message if an error occurs.

        Raises:
            Exception: If an error occurs during the question answering process.

        Note on Source Documents:
        The `return_source_documents=True` argument is used in the `RetrievalQA` chain.  However, due to a known issue in LangChain (at the time of writing), the `source_documents` are not reliably returned when using the "stuff" chain type. This might be fixed in later versions of LangChain.  If source documents are crucial, consider exploring other chain types (e.g., "map_reduce", "refine") or workarounds.

        Example:
            ```python
            from langchain.embeddings import OpenAIEmbeddings  # Example, use your embedding
            from langchain.vectorstores import FAISS  # Example, use your vectorstore

            embeddings = OpenAIEmbeddings()
            db = FAISS.from_texts(
                ["This is the 1st document", "This is the 2nd document"], embeddings
            )
            query = "What is the main topic?"
            answer = answer_question(query, db)
            print(f"Answer: {answer}")
            ```
        """

    try:
        # 1. Retrieve all documents (modify based on your vector store)
        # This example is highly dependant on the vectorstore you are using.
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

        documents = list(vectorstore.docstore._dict.values())  # Example, will not work for all vectorstores.

        # 2. Concatenate document content
        document_texts = [doc.page_content for doc in documents]
        combined_text = "\n\n".join(document_texts)

        # 3. Pass the concatenated string to the LLM
        prompt_template_str = """
                Context:
                {context}

                Question: {question}

                Answer:
                """
        prompt = PromptTemplate(template=prompt_template_str, input_variables=["context", "question"])
        formatted_prompt = prompt.format(context=combined_text, question=query)

        response = llm.invoke(formatted_prompt)
        return response.content

    except Exception as e:
        logger.error(f"Error processing all documents: {e}")
        return f"An error occurred: {e}"
